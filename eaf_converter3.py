# Jupyter Labç”¨ EAFãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›ã‚³ãƒ¼ãƒ‰ï¼ˆå®Œå…¨ç‰ˆï¼šéŸ³å£°åˆ‡ã‚Šå‡ºã—æ©Ÿèƒ½ä»˜ãã€ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ä¿å­˜å¯¾å¿œï¼‰
import xml.etree.ElementTree as ET
import os
import re
import shutil
import zipfile
from pathlib import Path
from typing import Dict, List, Optional
import time

# ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªå‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
AUDIO_LIBRARY = None
try:
    import librosa
    import soundfile as sf
    AUDIO_LIBRARY = 'librosa'
    print("âœ… librosa + soundfile ã‚’ä½¿ç”¨ã—ã¾ã™")
except ImportError:
    try:
        from pydub import AudioSegment
        AUDIO_LIBRARY = 'pydub'
        print("âœ… pydub ã‚’ä½¿ç”¨ã—ã¾ã™")
    except ImportError:
        try:
            import wave
            import numpy as np
            AUDIO_LIBRARY = 'wave'
            print("âœ… æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª wave ã‚’ä½¿ç”¨ã—ã¾ã™ï¼ˆWAVãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å¯¾å¿œï¼‰")
        except ImportError:
            AUDIO_LIBRARY = None
            print("âš ï¸ éŸ³å£°å‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãªã—ï¼ˆãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›ã®ã¿åˆ©ç”¨å¯èƒ½ï¼‰")

def get_desktop_path():
    """ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã®ãƒ‘ã‚¹ã‚’å–å¾—ï¼ˆOSåˆ¥å¯¾å¿œï¼‰"""
    import platform
    
    system = platform.system()
    
    if system == "Windows":
        desktop = os.path.join(os.path.expanduser("~"), "Desktop")
        if not os.path.exists(desktop):
            desktop = os.path.join(os.path.expanduser("~"), "ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—")
    elif system == "Darwin":
        desktop = os.path.join(os.path.expanduser("~"), "Desktop")
    else:
        desktop = os.path.join(os.path.expanduser("~"), "Desktop")
        if not os.path.exists(desktop):
            desktop = os.path.expanduser("~")
    
    return desktop

class EAFConverter:
    def __init__(self, eaf_file_path: str, wav_file_path: str = None):
        self.eaf_file_path = eaf_file_path
        self.wav_file_path = wav_file_path
        self.tree = None
        self.root = None
        self.time_slots = {}
        self.tiers = {}
        
        # éŸ³å£°å‡¦ç†ç”¨ã®å±æ€§
        self.audio_data = None
        self.sample_rate = None
        self.audio_available = False
    
    def load_audio(self):
        """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        if not self.wav_file_path:
            print("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›ã®ã¿å®Ÿè¡Œã—ã¾ã™ã€‚")
            return False
            
        if not os.path.exists(self.wav_file_path):
            print(f"éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.wav_file_path}")
            return False
            
        if AUDIO_LIBRARY is None:
            print("ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚éŸ³å£°åˆ†å‰²æ©Ÿèƒ½ã¯ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚")
            return False
            
        try:
            if AUDIO_LIBRARY == 'librosa':
                self.audio_data, self.sample_rate = librosa.load(self.wav_file_path, sr=None)
                print(f"éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {self.wav_file_path}")
                print(f"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‘¨æ³¢æ•°: {self.sample_rate}Hz, é•·ã•: {len(self.audio_data)/self.sample_rate:.2f}ç§’")
                
            elif AUDIO_LIBRARY == 'pydub':
                if self.wav_file_path.lower().endswith('.wav'):
                    self.audio_data = AudioSegment.from_wav(self.wav_file_path)
                elif self.wav_file_path.lower().endswith('.mp3'):
                    self.audio_data = AudioSegment.from_mp3(self.wav_file_path)
                else:
                    self.audio_data = AudioSegment.from_file(self.wav_file_path)
                self.sample_rate = self.audio_data.frame_rate
                print(f"éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {self.wav_file_path}")
                print(f"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‘¨æ³¢æ•°: {self.sample_rate}Hz, é•·ã•: {len(self.audio_data)/1000:.2f}ç§’")
                
            elif AUDIO_LIBRARY == 'wave':
                with wave.open(self.wav_file_path, 'rb') as wav_file:
                    self.sample_rate = wav_file.getframerate()
                    frames = wav_file.readframes(wav_file.getnframes())
                    self.audio_data = np.frombuffer(frames, dtype=np.int16)
                print(f"éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {self.wav_file_path}")
                print(f"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‘¨æ³¢æ•°: {self.sample_rate}Hz, é•·ã•: {len(self.audio_data)/self.sample_rate:.2f}ç§’")
                
            self.audio_available = True
            return True
            
        except Exception as e:
            print(f"éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def parse_eaf(self):
        """EAFãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æã™ã‚‹"""
        try:
            self.tree = ET.parse(self.eaf_file_path)
            self.root = self.tree.getroot()
            print(f"EAFãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {self.eaf_file_path}")
        except ET.ParseError as e:
            print(f"XMLãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            return False
        except FileNotFoundError:
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.eaf_file_path}")
            return False
            
        # ã‚¿ã‚¤ãƒ ã‚¹ãƒ­ãƒƒãƒˆã‚’å–å¾—
        time_order = self.root.find('TIME_ORDER')
        if time_order is not None:
            for time_slot in time_order.findall('TIME_SLOT'):
                slot_id = time_slot.get('TIME_SLOT_ID')
                time_value = time_slot.get('TIME_VALUE')
                self.time_slots[slot_id] = int(time_value) if time_value else 0
        
        print(f"ã‚¿ã‚¤ãƒ ã‚¹ãƒ­ãƒƒãƒˆæ•°: {len(self.time_slots)}")
        
        # ãƒ†ã‚£ã‚¢æƒ…å ±ã‚’è¡¨ç¤º
        print("\nåˆ©ç”¨å¯èƒ½ãªãƒ†ã‚£ã‚¢:")
        for tier in self.root.findall('TIER'):
            tier_id = tier.get('TIER_ID')
            print(f"  - {tier_id}")
            
        # ãƒ†ã‚£ã‚¢ã‚’å–å¾—
        for tier in self.root.findall('TIER'):
            tier_id = tier.get('TIER_ID')
            self.tiers[tier_id] = []
            
            # ALIGNABLE_ANNOTATIONã‚’ãƒã‚§ãƒƒã‚¯
            for annotation in tier.findall('.//ALIGNABLE_ANNOTATION'):
                start_id = annotation.get('TIME_SLOT_REF1')
                end_id = annotation.get('TIME_SLOT_REF2')
                value_elem = annotation.find('ANNOTATION_VALUE')
                value = value_elem.text if value_elem is not None else ""
                
                self.tiers[tier_id].append({
                    'start_time': self.time_slots.get(start_id, 0),
                    'end_time': self.time_slots.get(end_id, 0),
                    'value': value.strip() if value else "",
                    'type': 'ALIGNABLE'
                })
            
            # REF_ANNOTATIONã‚‚ãƒã‚§ãƒƒã‚¯
            for annotation in tier.findall('.//REF_ANNOTATION'):
                ref_id = annotation.get('ANNOTATION_REF')
                value_elem = annotation.find('ANNOTATION_VALUE')
                value = value_elem.text if value_elem is not None else ""
                
                # å‚ç…§å…ˆã®ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®æ™‚é–“ã‚’å–å¾—
                ref_start, ref_end = self._get_ref_time(ref_id)
                
                self.tiers[tier_id].append({
                    'start_time': ref_start,
                    'end_time': ref_end,
                    'value': value.strip() if value else "",
                    'type': 'REF',
                    'ref_id': ref_id
                })
            
            # é–‹å§‹æ™‚é–“ã§ã‚½ãƒ¼ãƒˆ
            self.tiers[tier_id].sort(key=lambda x: x['start_time'])
            print(f"  {tier_id}: {len(self.tiers[tier_id])} ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³")
        
        return True
    
    def _get_ref_time(self, ref_id: str) -> tuple:
        """REF_ANNOTATIONã®å‚ç…§å…ˆã®æ™‚é–“ã‚’å–å¾—"""
        for tier in self.root.findall('TIER'):
            for annotation in tier.findall('.//ALIGNABLE_ANNOTATION'):
                if annotation.get('ANNOTATION_ID') == ref_id:
                    start_id = annotation.get('TIME_SLOT_REF1')
                    end_id = annotation.get('TIME_SLOT_REF2')
                    return (self.time_slots.get(start_id, 0), self.time_slots.get(end_id, 0))
            
            for annotation in tier.findall('.//REF_ANNOTATION'):
                if annotation.get('ANNOTATION_ID') == ref_id:
                    nested_ref_id = annotation.get('ANNOTATION_REF')
                    if nested_ref_id:
                        return self._get_ref_time(nested_ref_id)
        
        return (0, 0)
    
    def _split_sentences_by_punctuation(self, text: str, morph: str, gloss: str, translation: str, start_time: int = 0, end_time: int = 0) -> List[Dict]:
        """æ–‡æœ«è¨˜å·ï¼ˆ.ã€?ã€!ï¼‰ã§æ–‡ã‚’åˆ†å‰²ã—ã€æ™‚é–“æƒ…å ±ã‚‚ä¿æŒ"""
        sentence_pattern = r'([.?!]+)'
        text_parts = re.split(sentence_pattern, text)
        
        sentences = []
        current_text = ""
        
        morph_words = morph.split() if morph else []
        gloss_words = gloss.split() if gloss else []
        
        morph_idx = 0
        gloss_idx = 0
        
        total_chars = len(text.replace('.', '').replace('?', '').replace('!', ''))
        current_chars = 0
        
        for part in text_parts:
            is_punctuation = bool(re.match(r'^[.?!]+$', part))
            
            if is_punctuation:
                current_text += part
                
                if current_text.strip():
                    clean_text = current_text.replace('.', '').replace('?', '').replace('!', '')
                    text_words = clean_text.split()
                    num_morphs = 0
                    for word in text_words:
                        morphs_in_word = len(re.split(r'[=-]', word))
                        num_morphs += morphs_in_word
                    
                    sent_morphs = morph_words[morph_idx:morph_idx + num_morphs] if morph_idx < len(morph_words) else []
                    sent_glosses = gloss_words[gloss_idx:gloss_idx + num_morphs] if gloss_idx < len(gloss_words) else []
                    
                    sentence_chars = len(clean_text)
                    if total_chars > 0 and start_time != end_time:
                        char_ratio = sentence_chars / total_chars
                        duration = end_time - start_time
                        sentence_start = start_time + int((current_chars / total_chars) * duration)
                        sentence_end = sentence_start + int(char_ratio * duration)
                    else:
                        sentence_start = start_time
                        sentence_end = end_time
                    
                    sentences.append({
                        'text': current_text.strip(),
                        'morph': ' '.join(sent_morphs),
                        'gloss': ' '.join(sent_glosses),
                        'translation': translation,
                        'start_time': sentence_start,
                        'end_time': sentence_end
                    })
                    
                    morph_idx += num_morphs
                    gloss_idx += num_morphs
                    current_chars += sentence_chars
                    current_text = ""
            
            elif part.strip():
                current_text += part
        
        if current_text.strip():
            remaining_morphs = morph_words[morph_idx:] if morph_idx < len(morph_words) else []
            remaining_glosses = gloss_words[gloss_idx:] if gloss_idx < len(gloss_words) else []
            
            sentence_chars = len(current_text.replace('.', '').replace('?', '').replace('!', ''))
            if total_chars > 0 and start_time != end_time:
                char_ratio = sentence_chars / total_chars
                duration = end_time - start_time
                sentence_start = start_time + int((current_chars / total_chars) * duration)
                sentence_end = sentence_start + int(char_ratio * duration)
            else:
                sentence_start = start_time
                sentence_end = end_time
            
            sentences.append({
                'text': current_text.strip(),
                'morph': ' '.join(remaining_morphs),
                'gloss': ' '.join(remaining_glosses),
                'translation': translation,
                'start_time': sentence_start,
                'end_time': sentence_end
            })
        
        return sentences if sentences else [{
            'text': text, 
            'morph': morph, 
            'gloss': gloss, 
            'translation': translation,
            'start_time': start_time,
            'end_time': end_time
        }]
    
    def extract_sentences(self, tier_names: Dict[str, str] = None) -> List[Dict]:
        """æ–‡ã”ã¨ã«text, morph, gloss, translation, æ™‚é–“æƒ…å ±ã‚’æŠ½å‡º"""
        if tier_names is None:
            tier_names = {
                'text': 'text@KS',
                'morph': 'morph@KS', 
                'gloss': 'gloss@KS',
                'translation': 'translation@KS'
            }
        
        sentences = []
        
        text_tier = self.tiers.get(tier_names['text'], [])
        morph_tier = self.tiers.get(tier_names['morph'], [])
        gloss_tier = self.tiers.get(tier_names['gloss'], [])
        translation_tier = self.tiers.get(tier_names['translation'], [])
        
        print(f"\næŠ½å‡ºå¯¾è±¡:")
        print(f"  text: {len(text_tier)} é …ç›®")
        print(f"  morph: {len(morph_tier)} é …ç›®")
        print(f"  gloss: {len(gloss_tier)} é …ç›®")
        print(f"  translation: {len(translation_tier)} é …ç›®")
        
        for i, text_annotation in enumerate(text_tier):
            if not text_annotation['value']:
                continue
                
            start_time = text_annotation['start_time']
            end_time = text_annotation['end_time']
            
            morph = self._find_overlapping_annotation(morph_tier, start_time, end_time)
            gloss = self._find_overlapping_annotation(gloss_tier, start_time, end_time)
            translation = self._find_overlapping_annotation(translation_tier, start_time, end_time)
            
            split_sentences = self._split_sentences_by_punctuation(
                text_annotation['value'], morph, gloss, translation, start_time, end_time
            )
            
            sentences.extend(split_sentences)
        
        print(f"\næŠ½å‡ºã•ã‚ŒãŸæ–‡æ•°: {len(sentences)}")
        return sentences
    
    def _find_overlapping_annotation(self, tier_data: List[Dict], start_time: int, end_time: int) -> str:
        """æŒ‡å®šã•ã‚ŒãŸæ™‚é–“ç¯„å›²ã¨é‡è¤‡ã™ã‚‹ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¦‹ã¤ã‘ã¦çµåˆ"""
        matching_annotations = []
        
        for annotation in tier_data:
            overlap_start = max(annotation['start_time'], start_time)
            overlap_end = min(annotation['end_time'], end_time)
            
            if overlap_start < overlap_end or (annotation['start_time'] == start_time and annotation['end_time'] == end_time):
                matching_annotations.append(annotation)
        
        matching_annotations.sort(key=lambda x: x['start_time'])
        return ' '.join([ann['value'] for ann in matching_annotations if ann['value']])
    
    def save_audio_segment(self, start_ms: int, end_ms: int, output_path: str, padding_ms: int = 100):
        """æŒ‡å®šã•ã‚ŒãŸæ™‚é–“ç¯„å›²ã®éŸ³å£°ã‚’ä¿å­˜"""
        if not self.audio_available:
            return False
            
        try:
            padded_start = max(0, start_ms - padding_ms)
            
            if AUDIO_LIBRARY == 'librosa':
                start_sample = int((padded_start / 1000.0) * self.sample_rate)
                end_sample = int((end_ms / 1000.0) * self.sample_rate)
                padded_end_sample = min(len(self.audio_data), end_sample + int((padding_ms / 1000.0) * self.sample_rate))
                
                audio_segment = self.audio_data[start_sample:padded_end_sample]
                sf.write(output_path, audio_segment, self.sample_rate)
                
            elif AUDIO_LIBRARY == 'pydub':
                padded_end = end_ms + padding_ms
                audio_segment = self.audio_data[padded_start:padded_end]
                audio_segment.export(output_path, format="wav")
                
            elif AUDIO_LIBRARY == 'wave':
                start_sample = int((padded_start / 1000.0) * self.sample_rate)
                end_sample = int((end_ms / 1000.0) * self.sample_rate)
                padded_end_sample = min(len(self.audio_data), end_sample + int((padding_ms / 1000.0) * self.sample_rate))
                
                audio_segment = self.audio_data[start_sample:padded_end_sample]
                
                with wave.open(output_path, 'wb') as wav_out:
                    wav_out.setnchannels(1)
                    wav_out.setsampwidth(2)
                    wav_out.setframerate(self.sample_rate)
                    wav_out.writeframes(audio_segment.tobytes())
            
            return True
            
        except Exception as e:
            print(f"éŸ³å£°ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def split_audio_to_desktop(self, sentences: List[Dict], folder_name: str = None, 
                              padding_ms: int = 100, create_zip: bool = False):
        """åˆ†å‰²ã•ã‚ŒãŸæ–‡ã®éŸ³å£°ã‚’ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã«ä¿å­˜ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚‚å«ã‚€ï¼‰"""
        if not self.audio_available:
            print("éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚éŸ³å£°åˆ†å‰²ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")
            return None
            
        desktop_path = get_desktop_path()
        print(f"ğŸ“ ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ãƒ‘ã‚¹: {desktop_path}")
        
        if not folder_name:
            base_name = Path(self.eaf_file_path).stem
            folder_name = f"{base_name}_sentences"
        
        output_path = Path(desktop_path) / folder_name
        if output_path.exists():
            timestamp = int(time.time())
            backup_path = Path(desktop_path) / f"{folder_name}_backup_{timestamp}"
            shutil.move(str(output_path), str(backup_path))
            print(f"ğŸ“¦ æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_path}")
        
        output_path.mkdir(parents=True, exist_ok=True)
        
        if not sentences:
            print("åˆ†å‰²ã™ã‚‹æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return None
        
        saved_files = []
        
        for i, sentence in enumerate(sentences, 1):
            if not sentence.get('start_time') or not sentence.get('end_time'):
                print(f"âš ï¸ æ–‡ {i} ã«æ™‚é–“æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                continue
                
            safe_text = re.sub(r'[^\w\s-]', '', sentence['text'][:30])
            safe_text = re.sub(r'\s+', '_', safe_text.strip())
            filename = f"{i:03d}_{safe_text}.wav"
            output_file = output_path / filename
            
            success = self.save_audio_segment(
                sentence['start_time'], 
                sentence['end_time'], 
                str(output_file),
                padding_ms
            )
            
            if success:
                saved_files.append({
                    'number': i,
                    'text': sentence['text'],
                    'start_time': sentence['start_time'],
                    'end_time': sentence['end_time'],
                    'duration': sentence['end_time'] - sentence['start_time'],
                    'file_path': str(output_file)
                })
                print(f"âœ… ä¿å­˜å®Œäº†: {filename} ({sentence['start_time']}ms - {sentence['end_time']}ms)")
            else:
                print(f"âŒ ä¿å­˜å¤±æ•—: {filename}")
        
        # GB4Eå½¢å¼ã®TeXãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        print("ğŸ“ GB4Eå½¢å¼ï¼ˆLeipzig.styå¯¾å¿œï¼‰ã®TeXãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆä¸­...")
        gb4e_content = self.to_gb4e_format(sentences)
        gb4e_file = output_path / 'sentences_gb4e_leipzig.tex'
        with open(gb4e_file, 'w', encoding='utf-8', newline='\n') as f:
            f.write(gb4e_content)
        print(f"âœ… GB4Eå½¢å¼ï¼ˆLeipzig.styå¯¾å¿œï¼‰ä¿å­˜: {gb4e_file.name}")
        
        # DOCå½¢å¼ã®TXTãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        print("ğŸ“„ DOCå½¢å¼ã®TXTãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆä¸­...")
        doc_content = self.to_doc_format(sentences)
        doc_file = output_path / 'sentences_doc.txt'
        with open(doc_file, 'w', encoding='utf-8') as f:
            f.write(doc_content)
        print(f"âœ… DOCå½¢å¼ä¿å­˜: {doc_file.name}")
        
        # çµæœã‚’ã¾ã¨ã‚ãŸãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        summary_file = output_path / 'audio_summary.txt'
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«åˆ†å‰²çµæœ\n")
            f.write("="*50 + "\n\n")
            f.write(f"å…ƒãƒ•ã‚¡ã‚¤ãƒ«: {self.eaf_file_path}\n")
            f.write(f"éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«: {self.wav_file_path}\n")
            f.write(f"ç·æ–‡æ•°: {len(saved_files)}\n")
            f.write(f"ä¿å­˜å ´æ‰€: {output_path}\n\n")
            f.write("ğŸ“ ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«:\n")
            f.write(f"  - GB4Eå½¢å¼ï¼ˆLeipzig.styå¯¾å¿œï¼‰: {gb4e_file.name}\n")
            f.write(f"  - DOCå½¢å¼: {doc_file.name}\n")
            f.write(f"  - éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«: {len(saved_files)}å€‹\n\n")
            
            for file_info in saved_files:
                f.write(f"{file_info['number']:03d}. {file_info['text']}\n")
                f.write(f"     æ™‚é–“: {file_info['start_time']}ms - {file_info['end_time']}ms "
                       f"(é•·ã•: {file_info['duration']}ms)\n")
                f.write(f"     ãƒ•ã‚¡ã‚¤ãƒ«: {Path(file_info['file_path']).name}\n\n")
        
        # READMEãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        readme_file = output_path / 'README.txt'
        with open(readme_file, 'w', encoding='utf-8') as f:
            f.write("EAFãƒ•ã‚¡ã‚¤ãƒ«éŸ³å£°åˆ†å‰²çµæœï¼ˆLeipzig.styå¯¾å¿œç‰ˆï¼‰\n")
            f.write("="*40 + "\n\n")
            f.write("ğŸ“ ã“ã®ãƒ•ã‚©ãƒ«ãƒ€ã«ã¯ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå«ã¾ã‚Œã¦ã„ã¾ã™:\n\n")
            f.write("ğŸµ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«:\n")
            f.write(f"  - {len(saved_files)}å€‹ã®åˆ†å‰²ã•ã‚ŒãŸéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ« (001_*.wav ï½ {len(saved_files):03d}_*.wav)\n")
            f.write("  - å„ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ–‡å˜ä½ã§åˆ†å‰²ã•ã‚Œã¦ã„ã¾ã™\n\n")
            f.write("ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«:\n")
            f.write(f"  - {gb4e_file.name}: LaTeXç”¨gb4eå½¢å¼ã®ä¾‹æ–‡é›†ï¼ˆLeipzig.styå¯¾å¿œï¼‰\n")
            f.write(f"  - {doc_file.name}: ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã®ä¾‹æ–‡é›†\n")
            f.write(f"  - {summary_file.name}: è©³ç´°ãªåˆ†å‰²æƒ…å ±\n")
            f.write(f"  - {readme_file.name}: ã“ã®èª¬æ˜ãƒ•ã‚¡ã‚¤ãƒ«\n\n")
            f.write("ğŸ’¡ ä½¿ç”¨æ–¹æ³•:\n")
            f.write("  - éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«: å„æ–‡ã®éŸ³å£°ã‚’å€‹åˆ¥ã«å†ç”Ÿå¯èƒ½\n")
            f.write("  - GB4Eãƒ•ã‚¡ã‚¤ãƒ«: LaTeXã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦è¨€èªå­¦è«–æ–‡ç”¨ã®ä¾‹æ–‡é›†ã‚’ä½œæˆ\n")
            f.write("    \\usepackage{leipzig} ã‚’å¿˜ã‚Œãšã«è¿½åŠ ã—ã¦ãã ã•ã„\n")
            f.write("  - DOCãƒ•ã‚¡ã‚¤ãƒ«: ãã®ã¾ã¾æ–‡æ›¸ã«è²¼ã‚Šä»˜ã‘å¯èƒ½\n\n")
            f.write(f"ğŸ“… ä½œæˆæ—¥æ™‚: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ğŸ”§ å…ƒãƒ•ã‚¡ã‚¤ãƒ«: {Path(self.eaf_file_path).name}\n")
        
        # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã™ã‚‹å ´åˆ
        zip_file_path = None
        if create_zip and saved_files:
            zip_file_path = Path(desktop_path) / f"{folder_name}.zip"
            try:
                with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    for file_path in output_path.rglob('*'):
                        if file_path.is_file():
                            arcname = file_path.relative_to(output_path)
                            zipf.write(file_path, arcname)
                
                print(f"ğŸ“¦ ZIPãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†: {zip_file_path}")
            except Exception as e:
                print(f"âŒ ZIPä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
                zip_file_path = None
        
        if saved_files:
            print(f"\nğŸ‰ éŸ³å£°åˆ†å‰²å®Œäº†!")
            print(f"ğŸµ ä¿å­˜ã•ã‚ŒãŸéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(saved_files)}")
            print(f"ğŸ“ GB4Eå½¢å¼ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆLeipzig.styå¯¾å¿œï¼‰: {gb4e_file.name}")
            print(f"ğŸ“„ DOCå½¢å¼ãƒ•ã‚¡ã‚¤ãƒ«: {doc_file.name}")
            print(f"ğŸ“ ä¿å­˜å ´æ‰€: {output_path}")
            print(f"ğŸ“‹ è©³ç´°æƒ…å ±: {summary_file.name}")
            print(f"ğŸ’¡ ä½¿ã„æ–¹: {readme_file.name}")
            if zip_file_path:
                print(f"ğŸ“¦ ZIPãƒ•ã‚¡ã‚¤ãƒ«: {zip_file_path}")
        
        return {
            'saved_files': saved_files,
            'output_path': str(output_path),
            'gb4e_file': str(gb4e_file),
            'doc_file': str(doc_file),
            'summary_file': str(summary_file),
            'readme_file': str(readme_file),
            'zip_file': str(zip_file_path) if zip_file_path else None,
            'total_files': len(saved_files)
        }
    
    def _convert_ipa_to_tipa(self, text: str) -> str:
        """IPAæ–‡å­—ã‚’tipaãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®å½¢å¼ã«å¤‰æ›"""
        if not text:
            return text
            
        # IPAæ–‡å­—ã¨tipaã‚³ãƒãƒ³ãƒ‰ã®å¯¾å¿œè¡¨ï¼ˆ{}ã‚’è¿½åŠ ã—ã¦åŒºåˆ‡ã‚Šã‚’æ˜ç¢ºåŒ–ï¼‰
        ipa_to_tipa = {
            'É¨': '\\textbari{}',
            'É¯': '\\textturnm{}',
            'É›': '\\textepsilon{}',
            'É”': '\\textopeno{}',
            'Ã¦': '\\textae{}',
            'É‘': '\\textscripta{}',
            'É’': '\\textturnscripta{}',
            'É™': '\\textschwa{}',
            'Éª': '\\textsci{}',
            'ÊŠ': '\\textupsilon{}',
            'Êƒ': '\\textesh{}',
            'Ê’': '\\textyogh{}',
            'Î¸': '\\texttheta{}',
            'Ã°': '\\texteth{}',
            'Å‹': '\\texteng{}',
            'É²': '\\textltailn{}',
            'É³': '\\textrtailn{}',
            'É±': '\\textltailm{}',
            'É¾': '\\textfishhookr{}',
            'É½': '\\textrtailr{}',
            'É»': '\\textturnr{}',
            'É­': '\\textrtaill{}',
            'Ê': '\\textturny{}',
            'Êˆ': '\\textrtailt{}',
            'É–': '\\textrtaild{}',
            'Ê‚': '\\textrtails{}',
            'Ê': '\\textrtailz{}',
            'É•': '\\textctc{}',
            'Ê‘': '\\textctj{}',
            'Ã§': '\\textccedilla{}',
            'Ê': '\\textctj{}',
            'É£': '\\textgamma{}',
            'Ï‡': '\\textchi{}',
            'Ê': '\\textinvscr{}',
            'Ä§': '\\textcrh{}',
            'Ê•': '\\textrevglotstop{}',
            'Ê”': '\\textglotstop{}',
            'É¸': '\\textphi{}',
            'Î²': '\\textbeta{}',
            'Ê‹': '\\textscriptv{}',
            'É¹': '\\textturnr{}',
            'É°': '\\textturnmrleg{}',
            'Éº': '\\textlhti{}',
            'É¢': '\\textscg{}',
            'Ê›': '\\texthtg{}',
            'Ê„': '\\texthtbardotlessjdotlessj{}',
            'É ': '\\texthtg{}',
            'É¡': '\\textscg{}',
            'Ë': '\\textlengthmark{}',
            'Ëˆ': '\\textprimstress{}',
            'ËŒ': '\\textsecstress{}',
            'Ê²': '\\textpal{}',
            'Ê·': '\\textlab{}',
            'Ê°': '\\textsuperscript{h}',
            'â¿': '\\textsuperscript{n}',
            'Ê¼': '\\textglotstop{}',
        }
        
        result = text
        for ipa_char, tipa_command in ipa_to_tipa.items():
            result = result.replace(ipa_char, tipa_command)
        
        return result
    
    def _convert_tipa_back_to_ipa(self, text: str) -> str:
        """tipaã‚³ãƒãƒ³ãƒ‰ã‚’å…ƒã®IPAæ–‡å­—ã«æˆ»ã™"""
        if not text:
            return text
            
        result = text
        result = result.replace('\\textbari{}', 'É¨')
        result = result.replace('\\textturnm{}', 'É¯')
        result = result.replace('\\textepsilon{}', 'É›')
        result = result.replace('\\textopeno{}', 'É”')
        result = result.replace('\\textae{}', 'Ã¦')
        result = result.replace('\\textscripta{}', 'É‘')
        result = result.replace('\\textturnscripta{}', 'É’')
        result = result.replace('\\textschwa{}', 'É™')
        result = result.replace('\\textsci{}', 'Éª')
        result = result.replace('\\textupsilon{}', 'ÊŠ')
        result = result.replace('\\textesh{}', 'Êƒ')
        result = result.replace('\\textyogh{}', 'Ê’')
        result = result.replace('\\texttheta{}', 'Î¸')
        result = result.replace('\\texteth{}', 'Ã°')
        result = result.replace('\\texteng{}', 'Å‹')
        result = result.replace('\\textltailn{}', 'É²')
        result = result.replace('\\textrtailn{}', 'É³')
        result = result.replace('\\textltailm{}', 'É±')
        result = result.replace('\\textfishhookr{}', 'É¾')
        result = result.replace('\\textrtailr{}', 'É½')
        result = result.replace('\\textturnr{}', 'É»')
        result = result.replace('\\textrtaill{}', 'É­')
        result = result.replace('\\textturny{}', 'Ê')
        result = result.replace('\\textrtailt{}', 'Êˆ')
        result = result.replace('\\textrtaild{}', 'É–')
        result = result.replace('\\textrtails{}', 'Ê‚')
        result = result.replace('\\textrtailz{}', 'Ê')
        result = result.replace('\\textctc{}', 'É•')
        result = result.replace('\\textctj{}', 'Ê‘')
        result = result.replace('\\textccedilla{}', 'Ã§')
        result = result.replace('\\textgamma{}', 'É£')
        result = result.replace('\\textchi{}', 'Ï‡')
        result = result.replace('\\textinvscr{}', 'Ê')
        result = result.replace('\\textcrh{}', 'Ä§')
        result = result.replace('\\textrevglotstop{}', 'Ê•')
        result = result.replace('\\textglotstop{}', 'Ê”')
        result = result.replace('\\textphi{}', 'É¸')
        result = result.replace('\\textbeta{}', 'Î²')
        result = result.replace('\\textscriptv{}', 'Ê‹')
        result = result.replace('\\textturnmrleg{}', 'É°')
        result = result.replace('\\textlhti{}', 'Éº')
        result = result.replace('\\textscg{}', 'É¢')
        result = result.replace('\\texthtg{}', 'Ê›')
        result = result.replace('\\texthtbardotlessjdotlessj{}', 'Ê„')
        result = result.replace('\\textbardotlessj{}', 'ÉŸ')
        result = result.replace('\\textlengthmark{}', 'Ë')
        result = result.replace('\\textprimstress{}', 'Ëˆ')
        result = result.replace('\\textsecstress{}', 'ËŒ')
        result = result.replace('\\textpal{}', 'Ê²')
        result = result.replace('\\textlab{}', 'Ê·')
        result = result.replace('\\textsuperscript{h}', 'Ê°')
        result = result.replace('\\textsuperscript{n}', 'â¿')
        
        return result
    
    def _align_morphs_with_text(self, text: str, morph: str) -> str:
        """textå±¤ã®åŒºåˆ‡ã‚Šæ–‡å­—ï¼ˆ=ã‚„-ï¼‰ã«åŸºã¥ã„ã¦morphå±¤ã‚’å†é…ç½®"""
        if not text or not morph:
            return morph
        
        morph_list = morph.split()
        if not morph_list:
            return morph
        
        text_words = text.split()
        result_parts = []
        morph_idx = 0
        
        for word in text_words:
            segments = re.split(r'([=-])', word)
            word_morphs = []
            
            for segment in segments:
                if segment in ['=', '-']:
                    continue
                elif segment.strip():
                    if morph_idx < len(morph_list):
                        word_morphs.append(morph_list[morph_idx])
                        morph_idx += 1
            
            if word_morphs:
                morphs_with_delims = []
                morph_pos = 0
                
                for segment in segments:
                    if segment in ['=', '-']:
                        morphs_with_delims.append(segment)
                    elif segment.strip() and morph_pos < len(word_morphs):
                        morphs_with_delims.append(word_morphs[morph_pos])
                        morph_pos += 1
                
                result_parts.append(''.join(morphs_with_delims))
        
        return ' '.join(result_parts)
    
    def _align_words_for_doc(self, text_line: str, gloss_line: str) -> tuple:
        """docå½¢å¼ç”¨ã«å˜èªã®é–‹å§‹ä½ç½®ã‚’æƒãˆã‚‹"""
        if not text_line or not gloss_line:
            return text_line, gloss_line
        
        text_words = text_line.split()
        gloss_words = gloss_line.split()
        
        def char_width(s):
            import unicodedata
            width = 0
            for char in s:
                if unicodedata.east_asian_width(char) in ('F', 'W'):
                    width += 2
                elif unicodedata.east_asian_width(char) in ('H', 'Na', 'N'):
                    width += 1
                else:
                    width += 1
            return width
        
        min_len = min(len(text_words), len(gloss_words))
        if len(text_words) != len(gloss_words):
            print(f"è­¦å‘Š: å˜èªæ•°ãŒä¸€è‡´ã—ã¾ã›ã‚“ (text: {len(text_words)}, gloss: {len(gloss_words)})")
        
        aligned_text_parts = []
        aligned_gloss_parts = []
        
        for i in range(min_len):
            text_word = text_words[i]
            gloss_word = gloss_words[i]
            
            text_width = char_width(text_word)
            gloss_width = char_width(gloss_word)
            
            max_width = max(text_width, gloss_width) + 2
            
            text_padding = max_width - text_width
            gloss_padding = max_width - gloss_width
            
            if i < min_len - 1:
                aligned_text_parts.append(text_word + ' ' * text_padding)
                aligned_gloss_parts.append(gloss_word + ' ' * gloss_padding)
            else:
                aligned_text_parts.append(text_word)
                aligned_gloss_parts.append(gloss_word)
        
        if len(text_words) > min_len:
            remaining_text = ' '.join(text_words[min_len:])
            aligned_text_parts.append(' ' + remaining_text)
        
        if len(gloss_words) > min_len:
            remaining_gloss = ' '.join(gloss_words[min_len:])
            aligned_gloss_parts.append(' ' + remaining_gloss)
        
        return ''.join(aligned_text_parts), ''.join(aligned_gloss_parts)

    def _convert_leipzig_glosses(self, gloss_text: str) -> str:
        """Leipzig.styã®è¦å‰‡ã«å¾“ã£ã¦å¤§æ–‡å­—è‹±å­—ã®æ–‡æ³•å½¢æ…‹ç´ è¨˜å·ã‚’å¤‰æ›"""
        if not gloss_text:
            return gloss_text
        
        # ãƒãƒƒã‚¯ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’äºŒé‡ã«ã—ã¦ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
        leipzig_mapping = {
            'NOM': '\\\\textsc{nom}', 'ACC': '\\\\textsc{acc}', 'GEN': '\\\\textsc{gen}',
            'DAT': '\\\\textsc{dat}', 'ABL': '\\\\textsc{abl}', 'LOC': '\\\\textsc{loc}',
            'PST': '\\\\textsc{pst}', 'PRS': '\\\\textsc{prs}', 'FUT': '\\\\textsc{fut}',
            'NPST': '\\\\textsc{npst}', 'PFV': '\\\\textsc{pfv}', 'IPFV': '\\\\textsc{ipfv}',
            'SG': '\\\\textsc{sg}', 'PL': '\\\\textsc{pl}', 'DU': '\\\\textsc{du}',
            'COP': '\\\\textsc{cop}', 'AUX': '\\\\textsc{aux}', 'NEG': '\\\\textsc{neg}',
            'FOC': '\\\\textsc{foc}', 'TOP': '\\\\textsc{top}', 'EMPH': '\\\\textsc{emph}',
            'HS': '\\\\textsc{hs}', 'EVID': '\\\\textsc{evid}', 'QUOT': '\\\\textsc{quot}',
            'SFP': '\\\\textsc{sfp}', 'CAS': '\\\\textsc{cas}', 'PART': '\\\\textsc{part}',
            'CAUS': '\\\\textsc{caus}', 'PASS': '\\\\textsc{pass}', 'REFL': '\\\\textsc{refl}',
            'Q': '\\\\textsc{q}', 'CLF': '\\\\textsc{clf}', 'DET': '\\\\textsc{det}',
            'DEF': '\\\\textsc{def}', 'INDEF': '\\\\textsc{indef}', 'COM': '\\\\textsc{com}'
        }
        
        result = gloss_text
        
        # ã‚ˆã‚Šæ…é‡ã«å¤‰æ›ï¼šå˜èªå¢ƒç•Œã‚’å³å¯†ã«ãƒã‚§ãƒƒã‚¯
        for original, replacement in leipzig_mapping.items():
            pattern = r'(?<![A-Za-z])' + re.escape(original) + r'(?![A-Za-z])'
            result = re.sub(pattern, replacement, result)
        
        # æ®‹ã£ãŸé€£ç¶šã™ã‚‹å¤§æ–‡å­—ã‚’è‡ªå‹•å¤‰æ›ï¼ˆå‰å¾Œã«è‹±æ•°å­—ãŒãªã„å ´åˆã®ã¿ï¼‰
        def convert_unknown_caps(match):
            caps_text = match.group(0)
            return f'\\\\textsc{{{caps_text.lower()}}}'
        
        result = re.sub(r'(?<![A-Za-z])[A-Z]{2,}(?![A-Za-z])', convert_unknown_caps, result)
        
        return result
    
    def _convert_leipzig_back_to_plain(self, text: str) -> str:
        """Leipzig.styã®smallcapsã‚³ãƒãƒ³ãƒ‰ã‚’å…ƒã®å°å‹å¤§æ–‡å­—ã«æˆ»ã™"""
        if not text:
            return text
        
        def convert_textsc_to_smallcaps(match):
            content = match.group(1)
            # å°å‹å¤§æ–‡å­—ã«å¤‰æ›ï¼ˆå®Ÿéš›ã«ã¯Unicodeã®å°å‹å¤§æ–‡å­—æ–‡å­—ã‚’ä½¿ç”¨ï¼‰
            smallcap_mapping = {
                'nom': 'É´á´á´', 'acc': 'á´€á´„á´„', 'gen': 'É¢á´‡É´',
                'dat': 'á´…á´€á´›', 'abl': 'á´€Ê™ÊŸ', 'loc': 'ÊŸá´á´„',
                'pst': 'á´˜sá´›', 'prs': 'á´˜Ê€s', 'fut': 'êœ°á´œá´›',
                'npst': 'É´á´˜sá´›', 'pfv': 'á´˜êœ°á´ ', 'ipfv': 'Éªá´˜êœ°á´ ',
                'sg': 'sÉ¢', 'pl': 'á´˜ÊŸ', 'du': 'á´…á´œ',
                'cop': 'á´„á´á´˜', 'aux': 'á´€á´œx', 'neg': 'É´á´‡É¢',
                'foc': 'êœ°á´á´„', 'top': 'á´›á´á´˜', 'emph': 'á´‡á´á´˜Êœ',
                'hs': 'Êœs', 'evid': 'á´‡á´ Éªá´…', 'quot': 'Qá´œá´á´›',
                'sfp': 'sêœ°á´˜', 'cas': 'á´„á´€s', 'part': 'á´˜á´€Ê€á´›',
                'caus': 'á´„á´€á´œs', 'pass': 'á´˜á´€ss', 'refl': 'Ê€á´‡êœ°ÊŸ',
                'q': 'Q', 'clf': 'á´„ÊŸêœ°', 'det': 'á´…á´‡á´›',
                'def': 'á´…á´‡êœ°', 'indef': 'ÉªÉ´á´…á´‡êœ°', 'com': 'á´„á´á´'
            }
            
            return smallcap_mapping.get(content.lower(), content.upper())
        
        # \\textsc{...} ã‚’å°å‹å¤§æ–‡å­—ã«å¤‰æ›
        result = re.sub(r'\\textsc\{([^}]+)\}', convert_textsc_to_smallcaps, text)
        
        # é€šå¸¸ã®å¤§æ–‡å­—ï¼ˆ2æ–‡å­—ä»¥ä¸Šï¼‰ã‚‚å°å‹å¤§æ–‡å­—ã«å¤‰æ›
        def convert_caps_to_smallcaps(match):
            caps_text = match.group(0)
            result_chars = []
            for char in caps_text:
                # å€‹åˆ¥æ–‡å­—ã®ãƒãƒƒãƒ”ãƒ³ã‚°
                char_mapping = {
                    'A': 'á´€', 'B': 'Ê™', 'C': 'á´„', 'D': 'á´…', 'E': 'á´‡', 'F': 'êœ°',
                    'G': 'É¢', 'H': 'Êœ', 'I': 'Éª', 'J': 'á´Š', 'K': 'á´‹', 'L': 'ÊŸ',
                    'M': 'á´', 'N': 'É´', 'O': 'á´', 'P': 'á´˜', 'Q': 'Q', 'R': 'Ê€',
                    'S': 's', 'T': 'á´›', 'U': 'á´œ', 'V': 'á´ ', 'W': 'á´¡', 'X': 'x',
                    'Y': 'Ê', 'Z': 'á´¢'
                }
                result_chars.append(char_mapping.get(char, char))
            return ''.join(result_chars)
        
        # é€£ç¶šã™ã‚‹å¤§æ–‡å­—ï¼ˆ2æ–‡å­—ä»¥ä¸Šï¼‰ã‚’å°å‹å¤§æ–‡å­—ã«å¤‰æ›
        result = re.sub(r'(?<![A-Za-z])[A-Z]{2,}(?![A-Za-z])', convert_caps_to_smallcaps, result)
        
        return result
    
    def to_gb4e_format(self, sentences: List[Dict]) -> str:
        """gb4eå½¢å¼ã«å¤‰æ›ï¼ˆLeipzig.styå¯¾å¿œã€IPAâ†’tipaå¤‰æ›ä»˜ãï¼‰"""
        output = []
        
        output.append("% UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç”¨è¨­å®š")
        output.append("% \\usepackage[utf8]{inputenc}")
        output.append("% \\usepackage{CJKutf8}")
        output.append("% \\usepackage{gb4e}")
        output.append("% \\usepackage{tipa}")
        output.append("% \\usepackage{leipzig}  % Leipzig.styãƒ‘ãƒƒã‚±ãƒ¼ã‚¸")
        output.append("")
        output.append("% Leipzig.styã®ä½¿ç”¨ã«ã‚ˆã‚Šã€å¤§æ–‡å­—ã®æ–‡æ³•è¨˜å·ãŒè‡ªå‹•çš„ã«å°æ–‡å­—ã®ã‚¹ãƒ¢ãƒ¼ãƒ«ã‚­ãƒ£ãƒƒãƒ—ã‚¹ã«å¤‰æ›ã•ã‚Œã¾ã™")
        output.append("% IPAæ–‡å­—ã¯è‡ªå‹•çš„ã«tipaã‚³ãƒãƒ³ãƒ‰ã«å¤‰æ›ã•ã‚Œã¾ã™")
        output.append("")
        
        for i, sentence in enumerate(sentences, 1):
            if not sentence['text']:
                continue
                
            output.append("\\begin{exe}")
            output.append("\\ex")
            
            # 1æ®µç›®: textï¼ˆIPAã‚’tipaã«å¤‰æ›ï¼‰
            text_tipa = self._convert_ipa_to_tipa(sentence['text'])
            output.append(f"\\gll {text_tipa}\\\\")
            
            # 2æ®µç›®: glossï¼ˆå½¢æ…‹ç´ æ•´åˆ— + Leipzig.styå¤‰æ›ï¼‰
            if sentence['gloss']:
                # textå±¤ã®å¢ƒç•Œè¨˜å·ã«åŸºã¥ã„ã¦glosså±¤ã‚’æ•´åˆ—
                aligned_gloss = self._align_morphs_with_text(sentence['text'], sentence['gloss'])
                leipzig_gloss = self._convert_leipzig_glosses(aligned_gloss)
                # äºŒé‡ãƒãƒƒã‚¯ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’å˜ä¸€ã«ä¿®æ­£
                leipzig_gloss = leipzig_gloss.replace('\\\\', '\\')
                output.append(f"     {leipzig_gloss}\\\\")
            else:
                output.append("     \\\\")
            
            # 3æ®µç›®: translationï¼ˆãƒ‡ãƒãƒƒã‚°æƒ…å ±ä»˜ãï¼‰
            if sentence.get('translation') and sentence['translation'].strip():
                output.append(f"\\glt {sentence['translation']}")
            else:
                # ç¿»è¨³ãŒãªã„å ´åˆã®æƒ…å ±è¡¨ç¤º
                if not sentence.get('translation'):
                    print(f"è­¦å‘Š: æ–‡ {i} ã«ç¿»è¨³ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                else:
                    print(f"è­¦å‘Š: æ–‡ {i} ã®ç¿»è¨³ãŒç©ºã§ã™: '{sentence['translation']}'")
                output.append("\\glt")
            
            output.append("\\end{exe}")
            output.append("")
        
        return "\n".join(output)
    
    def to_doc_format(self, sentences: List[Dict], debug: bool = False) -> str:
        """docå½¢å¼ï¼ˆãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€IPAæ–‡å­—å¾©å…ƒã€å°å‹å¤§æ–‡å­—å¤‰æ›ã€ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆèª¿æ•´ä»˜ãï¼‰"""
        output = []
        
        for i, sentence in enumerate(sentences, 1):
            if not sentence['text']:
                continue
                
            output.append(f"({i})")
            
            # 1æ®µç›®: textï¼ˆIPAã‚’tipaã«å¤‰æ›ã—ã¦ã‹ã‚‰å…ƒã«æˆ»ã™ï¼‰
            text_tipa = self._convert_ipa_to_tipa(sentence['text'])
            text_original = self._convert_tipa_back_to_ipa(text_tipa)
            
            # 2æ®µç›®: glossï¼ˆåŒºåˆ‡ã‚Šæ–‡å­—ã«åŸºã¥ãæ•´åˆ— + Leipzig.styå¤‰æ›å¾Œã«å°å‹å¤§æ–‡å­—ã«æˆ»ã™ï¼‰
            if sentence['gloss']:
                aligned_gloss = self._align_morphs_with_text(sentence['text'], sentence['gloss'])
                leipzig_gloss = self._convert_leipzig_glosses(aligned_gloss)
                # äºŒé‡ãƒãƒƒã‚¯ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’å˜ä¸€ã«ä¿®æ­£
                leipzig_gloss = leipzig_gloss.replace('\\\\', '\\')
                plain_gloss = self._convert_leipzig_back_to_plain(leipzig_gloss)
                
                if debug:
                    print(f"\n--- ä¾‹æ–‡ {i} ã®ãƒ‡ãƒãƒƒã‚°æƒ…å ± ---")
                    print(f"å…ƒã®text: '{sentence['text']}'")
                    print(f"å…ƒã®gloss: '{sentence['gloss']}'")
                    print(f"æ•´åˆ—å¾Œgloss: '{aligned_gloss}'")
                    print(f"Leipzigå¤‰æ›å¾Œ: '{leipzig_gloss}'")
                    print(f"æœ€çµ‚text: '{text_original}'")
                    print(f"æœ€çµ‚gloss: '{plain_gloss}'")
                
                # å˜èªã®é–‹å§‹ä½ç½®ã‚’æƒãˆã‚‹
                aligned_text, aligned_gloss_final = self._align_words_for_doc(text_original, plain_gloss)
                
                if debug:
                    print(f"ä½ç½®èª¿æ•´å¾Œtext: '{aligned_text}'")
                    print(f"ä½ç½®èª¿æ•´å¾Œgloss: '{aligned_gloss_final}'")
                    print(f"textå˜èªæ•°: {len(text_original.split())}")
                    print(f"glosså˜èªæ•°: {len(plain_gloss.split())}")
                
                output.append(aligned_text)
                output.append(aligned_gloss_final)
            else:
                output.append(text_original)
                output.append("")
            
            # 3æ®µç›®: translation
            if sentence['translation']:
                output.append(sentence['translation'])
            else:
                output.append("")
            
            output.append("")
        
        return "\n".join(output)

# Leipzig.styãƒ†ã‚¹ãƒˆé–¢æ•°ã‚’æ”¹è‰¯
def test_leipzig_conversion():
    """Leipzig.styå¤‰æ›æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
    converter = EAFConverter("dummy.eaf")
    
    test_glosses = [
        "FOC",
        "GEN", 
        "COM",
        "PST",
        "HS",
        "æ˜”=FOC è²§ã—ã„=GEN",
        "UNKNOWN CUSTOM"
    ]
    
    print("=== Leipzig.styå¤‰æ›ãƒ†ã‚¹ãƒˆ ===")
    print("å…¥åŠ› â†’ å‡ºåŠ›")
    print("-" * 50)
    
    for gloss in test_glosses:
        converted = converter._convert_leipzig_glosses(gloss)
        print(f"'{gloss}' â†’ '{converted}'")
        
        # ãƒãƒƒã‚¯ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã®ç¢ºèª
        if '\\textsc' in converted:
            print("  âœ… \\textsc ãŒæ­£ã—ãå«ã¾ã‚Œã¦ã„ã¾ã™")
        elif 'extsc' in converted:
            print("  âŒ \\t ãŒæ¬ ã‘ã¦ã„ã¾ã™ï¼")
    
    print("\n" + "="*50)
    print("å¤‰æ›è¦å‰‡:")
    print("- å¤§æ–‡å­—ã®æ–‡æ³•è¨˜å·ã¯ \\textsc{å°æ–‡å­—} ã«å¤‰æ›")
    print("- æ•°å­—(1,2,3)ã¯ãã®ã¾ã¾ä¿æŒ")
    print("- æœªå®šç¾©ã®å¤§æ–‡å­—è¨˜å·ã‚‚è‡ªå‹•å¤‰æ›")

# è¨ºæ–­é–¢æ•°
def diagnose_eaf_file(eaf_filename, wav_filename=None):
    """EAFãƒ•ã‚¡ã‚¤ãƒ«ã®æ§‹é€ ã‚’è©³ã—ãèª¿ã¹ã‚‹"""
    converter = EAFConverter(eaf_filename, wav_filename)
    if not converter.parse_eaf():
        return
    
    if wav_filename:
        print("\néŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚§ãƒƒã‚¯...")
        converter.load_audio()
    
    print("\n=== è©³ç´°ãªãƒ†ã‚£ã‚¢æƒ…å ± ===")
    for tier_name, annotations in converter.tiers.items():
        print(f"\nãƒ†ã‚£ã‚¢: {tier_name}")
        print(f"  ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ•°: {len(annotations)}")
        
        for i, ann in enumerate(annotations[:3]):
            ann_type = ann.get('type', 'UNKNOWN')
            print(f"  [{i+1}] ({ann_type}) æ™‚é–“: {ann['start_time']}-{ann['end_time']}")
            content = ann['value'][:200] + '...' if len(ann['value']) > 200 else ann['value']
            print(f"      å†…å®¹: '{content}'")
            # å¢ƒç•Œè¨˜å·ã®ç¢ºèª
            if '=' in ann['value'] or '-' in ann['value']:
                print(f"      *** å¢ƒç•Œè¨˜å·ã‚’æ¤œå‡º: = ã‚„ - ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ ***")
        
        if len(annotations) > 3:
            print(f"  ... (ä»– {len(annotations)-3} å€‹)")

# æŠ½å‡ºå†…å®¹ç¢ºèªç”¨ã®é–¢æ•°ã‚’è¿½åŠ 
def debug_sentence_extraction(eaf_filename, tier_names=None):
    """æ–‡æŠ½å‡ºãƒ—ãƒ­ã‚»ã‚¹ã‚’è©³ã—ãç¢ºèª"""
    converter = EAFConverter(eaf_filename)
    if not converter.parse_eaf():
        return
    
    sentences = converter.extract_sentences(tier_names)
    
    print("\n=== æŠ½å‡ºã•ã‚ŒãŸæ–‡ã®è©³ç´°ç¢ºèª ===")
    for i, sentence in enumerate(sentences[:3], 1):  # æœ€åˆã®3æ–‡ã®ã¿
        print(f"\n--- æ–‡ {i} ---")
        print(f"text: '{sentence['text']}'")
        print(f"morph: '{sentence['morph']}'")
        print(f"gloss: '{sentence['gloss']}'")
        print(f"translation: '{sentence['translation']}'")
        print(f"æ™‚é–“: {sentence['start_time']}ms - {sentence['end_time']}ms")
        
        # å¢ƒç•Œè¨˜å·ã®ç¢ºèª
        if '=' in sentence['text'] or '-' in sentence['text']:
            print("  *** textå±¤ã«å¢ƒç•Œè¨˜å·ã‚ã‚Š ***")
        if '=' in sentence['gloss'] or '-' in sentence['gloss']:
            print("  *** glosså±¤ã«å¢ƒç•Œè¨˜å·ã‚ã‚Š ***")
        if '=' in sentence['morph'] or '-' in sentence['morph']:
            print("  *** morphå±¤ã«å¢ƒç•Œè¨˜å·ã‚ã‚Š ***")
        
        # ç¿»è¨³ã®è©³ç´°ç¢ºèª
        if not sentence.get('translation'):
            print("  âŒ translation ã‚­ãƒ¼ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
        elif not sentence['translation']:
            print("  âŒ translation ãŒç©ºæ–‡å­—åˆ—ã§ã™")
        elif not sentence['translation'].strip():
            print("  âŒ translation ãŒç©ºç™½ã®ã¿ã§ã™")
        else:
            print(f"  âœ… translation OK: '{sentence['translation']}'")
    
    if len(sentences) > 3:
        print(f"\n... (ä»– {len(sentences)-3} æ–‡)")
    
    # ãƒ†ã‚£ã‚¢åã®ç¢ºèªã‚‚è¿½åŠ 
    if tier_names:
        print(f"\n=== ä½¿ç”¨ä¸­ã®ãƒ†ã‚£ã‚¢å ===")
        for key, value in tier_names.items():
            print(f"{key}: '{value}'")
    else:
        print(f"\n=== ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ†ã‚£ã‚¢åä½¿ç”¨ ===")
        default_tiers = {
            'text': 'text@KS',
            'morph': 'morph@KS', 
            'gloss': 'gloss@KS',
            'translation': 'translation@KS'
        }
        for key, value in default_tiers.items():
            print(f"{key}: '{value}'")

# morphæ•´åˆ—ã®ãƒ†ã‚¹ãƒˆé–¢æ•°ã‚’è¿½åŠ 
def test_morph_alignment():
    """morphæ•´åˆ—æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
    converter = EAFConverter("dummy.eaf")
    
    test_cases = [
        {
            'text': 'nkjaan=du kiban-kiban=nu',
            'morph': 'æ˜” FOC è²§ã—ã„ è²§ã—ã„ GEN'
        },
        {
            'text': 'pZtu=tu ujaki=ti=nu',
            'morph': 'äºº COM è±Šã‹ãª QUOT GEN'
        },
        {
            'text': 'a-tar=ca',
            'morph': 'COP PST HS'
        }
    ]
    
    print("=== morphæ•´åˆ—ãƒ†ã‚¹ãƒˆ ===")
    for i, case in enumerate(test_cases, 1):
        print(f"\n--- ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ {i} ---")
        print(f"å…¥åŠ›text: '{case['text']}'")
        print(f"å…¥åŠ›morph: '{case['morph']}'")
        
        result = converter._align_morphs_with_text(case['text'], case['morph'])
        print(f"æ•´åˆ—çµæœ: '{result}'")
        
        leipzig_result = converter._convert_leipzig_glosses(result)
        print(f"Leipzigå¤‰æ›: '{leipzig_result}'")

# å¤‰æ›é–¢æ•°
def convert_eaf_file(eaf_filename, wav_filename=None, tier_names=None, output_format='both',
                    debug=False, save_audio=True, audio_folder_name=None,
                    audio_padding_ms=100, create_zip=False):
    """
    EAFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¤‰æ›ã™ã‚‹é–¢æ•°ï¼ˆLeipzig.styå¯¾å¿œï¼‹éŸ³å£°åˆ‡ã‚Šå‡ºã—æ©Ÿèƒ½ä»˜ãï¼‰
    
    Args:
        eaf_filename: EAFãƒ•ã‚¡ã‚¤ãƒ«å
        wav_filename: WAVãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆéŸ³å£°åˆ‡ã‚Šå‡ºã—ç”¨ã€ä»»æ„ï¼‰
        tier_names: ãƒ†ã‚£ã‚¢åã®è¾æ›¸ {'text': 'å®Ÿéš›ã®ãƒ†ã‚£ã‚¢å', ...}
        output_format: 'gb4e', 'doc', 'both'
        debug: ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤ºã™ã‚‹ã‹ã©ã†ã‹
        save_audio: éŸ³å£°åˆ†å‰²ã‚’å®Ÿè¡Œã™ã‚‹ã‹ã©ã†ã‹
        audio_folder_name: éŸ³å£°ä¿å­˜ç”¨ãƒ•ã‚©ãƒ«ãƒ€å
        audio_padding_ms: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰å¾Œãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆãƒŸãƒªç§’ï¼‰
        create_zip: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ZIPã‚’ä½œæˆã™ã‚‹ã‹ã©ã†ã‹
    
    Returns:
        å¤‰æ›çµæœã®è¾æ›¸
    """
    
    if not os.path.exists(eaf_filename):
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {eaf_filename}")
        print("\nç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«:")
        for file in os.listdir('.'):
            if file.endswith('.eaf'):
                print(f"  {file}")
        return None
    
    if wav_filename and not os.path.exists(wav_filename):
        print(f"éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {wav_filename}")
        print("ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›ã®ã¿å®Ÿè¡Œã—ã¾ã™ã€‚")
        wav_filename = None
    
    converter = EAFConverter(eaf_filename, wav_filename)
    
    if not converter.parse_eaf():
        return None
    
    if wav_filename:
        print("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        converter.load_audio()
    
    sentences = converter.extract_sentences(tier_names)
    
    if not sentences:
        print("å¤‰æ›å¯èƒ½ãªæ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None
    
    result = {
        'sentences': sentences,
        'eaf_file': eaf_filename,
        'wav_file': wav_filename,
        'gb4e_file': None,
        'doc_file': None,
        'audio_result': None
    }
    
    print("\n" + "="*70)
    
    if output_format in ['gb4e', 'both']:
        print("GB4Eå½¢å¼ (Leipzig.styå¯¾å¿œ):")
        print("-" * 40)
        gb4e_content = converter.to_gb4e_format(sentences)
        print(gb4e_content)
        
        gb4e_filename = f"{eaf_filename}_gb4e_leipzig.tex"
        with open(gb4e_filename, 'w', encoding='utf-8', newline='\n') as f:
            f.write(gb4e_content)
        print(f"\nâœ… GB4Eå½¢å¼(Leipzig.styå¯¾å¿œ)ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {gb4e_filename}")
        result['gb4e_file'] = gb4e_filename
    
    if output_format in ['both']:
        print("\n" + "="*70)
    
    if output_format in ['doc', 'both']:
        print("DOCå½¢å¼:")
        print("-" * 40)
        doc_content = converter.to_doc_format(sentences)
        print(doc_content)
        
        doc_filename = f"{eaf_filename}_doc.txt"
        with open(doc_filename, 'w', encoding='utf-8') as f:
            f.write(doc_content)
        print(f"\nâœ… DOCå½¢å¼ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {doc_filename}")
        result['doc_file'] = doc_filename
    
    # éŸ³å£°åˆ†å‰²ã‚’å®Ÿè¡Œ
    if save_audio and wav_filename and converter.audio_available:
        print("\n" + "="*70)
        print("ğŸµ éŸ³å£°åˆ†å‰²ã‚’å®Ÿè¡Œä¸­...")
        audio_result = converter.split_audio_to_desktop(
            sentences, audio_folder_name, audio_padding_ms, create_zip
        )
        result['audio_result'] = audio_result
    elif save_audio and wav_filename:
        print("\néŸ³å£°åˆ†å‰²: ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸ")
    elif save_audio:
        print("\néŸ³å£°åˆ†å‰²: WAVãƒ•ã‚¡ã‚¤ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸ")
    
    print(f"\nğŸ‰ å¤‰æ›å®Œäº†!")
    print(f"ğŸ“Š æŠ½å‡ºã•ã‚ŒãŸæ–‡æ•°: {len(sentences)}")
    if result['gb4e_file']:
        print(f"ğŸ“ GB4Eå½¢å¼ï¼ˆLeipzig.styå¯¾å¿œï¼‰: {result['gb4e_file']}")
    if result['doc_file']:
        print(f"ğŸ“„ DOCå½¢å¼: {result['doc_file']}")
    if result['audio_result']:
        print(f"ğŸµ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«: {result['audio_result']['total_files']}å€‹")
        print(f"ğŸ“ éŸ³å£°ä¿å­˜å ´æ‰€: {result['audio_result']['output_path']}")
    
    return result

# å®Ÿè¡Œæ–¹æ³•ã®èª¬æ˜
print("=== EAFãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›ãƒ„ãƒ¼ãƒ«ï¼ˆLeipzig.styå¯¾å¿œï¼‹éŸ³å£°åˆ‡ã‚Šå‡ºã—æ©Ÿèƒ½ä»˜ãï¼‰ ===")
print("æ–°æ©Ÿèƒ½:")
print("âœ… Leipzig.styãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã«å¯¾å¿œã—ãŸæ–‡æ³•è¨˜å·ã®è‡ªå‹•å¤‰æ›") 
print("âœ… NOM â†’ \\textsc{nom}, PST â†’ \\textsc{pst} ãªã©")
print("âœ… æœªå®šç¾©ã®å¤§æ–‡å­—è¨˜å·ã‚‚è‡ªå‹•ã§smallcapsã«å¤‰æ›")
print("âœ… IPAæ–‡å­—ã‚’tipaã‚³ãƒãƒ³ãƒ‰ã«è‡ªå‹•å¤‰æ›ï¼ˆÉ› â†’ \\textepsilon{}, Å‹ â†’ \\texteng{} ãªã©ï¼‰")
print("âœ… éŸ³å£°åˆ†å‰²ï¼šæ–‡å˜ä½ã§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†å‰²ã—ã¦ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã«ä¿å­˜")
print("âœ… ZIPãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼šéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã¾ã¨ã‚ã¦ZIPåœ§ç¸®")
print("âœ… æ–‡æœ«è¨˜å·ã§ã®è‡ªå‹•æ–‡åˆ†å‰²ï¼ˆæ™‚é–“æƒ…å ±ä»˜ãï¼‰")
print("âœ… å½¢æ…‹ç´ æ•´åˆ—ï¼štextå±¤ã®åŒºåˆ‡ã‚Šæ–‡å­—ã«åŸºã¥ã„ã¦morph/glosså±¤ã‚’å†é…ç½®")
print()

if AUDIO_LIBRARY:
    print("ä½¿ç”¨æ–¹æ³•:")
    print("1. ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›ã®ã¿:")
    print("   result = convert_eaf_file('your_file.eaf')")
    print()
    print("2. ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›ï¼‹éŸ³å£°åˆ†å‰²:")
    print("   result = convert_eaf_file('your_file.eaf', 'your_file.wav')")
    print()
    print("3. ã‚«ã‚¹ã‚¿ãƒ ãƒ†ã‚£ã‚¢åã‚’æŒ‡å®š:")
    print("   tier_names = {")
    print("       'text': 'text@Speaker1',      # å®Ÿéš›ã®textå±¤ã®åå‰")
    print("       'morph': 'morph@Speaker1',    # å®Ÿéš›ã®morphå±¤ã®åå‰")
    print("       'gloss': 'gloss@Speaker1',    # å®Ÿéš›ã®glosså±¤ã®åå‰")
    print("       'translation': 'translation@Speaker1'  # å®Ÿéš›ã®translationå±¤ã®åå‰")
    print("   }")
    print("   result = convert_eaf_file('your_file.eaf', 'your_file.wav',")
    print("                            tier_names=tier_names)")
    print()
    print("4. å®Œå…¨ã‚«ã‚¹ã‚¿ãƒ ï¼ˆãƒ†ã‚£ã‚¢åæŒ‡å®šï¼‹å…¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰:")
    print("   tier_names = {'text': 'text@MyName', 'gloss': 'gloss@MyName', ...}")
    print("   result = convert_eaf_file('your_file.eaf', 'your_file.wav',")
    print("                            tier_names=tier_names,")
    print("                            output_format='both',")
    print("                            save_audio=True,")
    print("                            audio_folder_name='My_Audio_Project',")
    print("                            create_zip=True)")
    print()
    print("5. Leipzig.styå¤‰æ›ãƒ†ã‚¹ãƒˆ:")
    print("   test_leipzig_conversion()")
    print()
    print("6. morphæ•´åˆ—ãƒ†ã‚¹ãƒˆ:")
    print("   test_morph_alignment()")
    print()
    print("7. æ–‡æŠ½å‡ºãƒ‡ãƒãƒƒã‚°:")
    print("   debug_sentence_extraction('your_file.eaf')")
    print()
    print("8. è¨ºæ–­å®Ÿè¡Œï¼ˆãƒ†ã‚£ã‚¢åç¢ºèªã«ä¾¿åˆ©ï¼‰:")
    print("   diagnose_eaf_file('your_file.eaf', 'your_file.wav')")
else:
    print("éŸ³å£°å‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›ã®ã¿åˆ©ç”¨å¯èƒ½ã§ã™ã€‚")
    print("éŸ³å£°åˆ†å‰²æ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ä»¥ä¸‹ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
    print("  pip install librosa soundfile  # æ¨å¥¨")
    print("  pip install pydub              # è»½é‡ç‰ˆ")
    print()
    print("ä½¿ç”¨æ–¹æ³•ï¼ˆãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›ã®ã¿ï¼‰:")
    print("1. åŸºæœ¬çš„ãªå¤‰æ›:")
    print("   result = convert_eaf_file('your_file.eaf')")
    print()
    print("2. ã‚«ã‚¹ã‚¿ãƒ ãƒ†ã‚£ã‚¢åã‚’æŒ‡å®š:")
    print("   tier_names = {")
    print("       'text': 'text@Speaker1',")
    print("       'morph': 'morph@Speaker1',")
    print("       'gloss': 'gloss@Speaker1',")
    print("       'translation': 'translation@Speaker1'")
    print("   }")
    print("   result = convert_eaf_file('your_file.eaf', tier_names=tier_names)")
    print()
    print("3. Leipzig.styå¤‰æ›ãƒ†ã‚¹ãƒˆ:")
    print("   test_leipzig_conversion()")
    print()
    print("4. morphæ•´åˆ—ãƒ†ã‚¹ãƒˆ:")
    print("   test_morph_alignment()")
    print()
    print("5. æ–‡æŠ½å‡ºãƒ‡ãƒãƒƒã‚°:")
    print("   debug_sentence_extraction('your_file.eaf')")
    print()
    print("6. è¨ºæ–­å®Ÿè¡Œï¼ˆãƒ†ã‚£ã‚¢åç¢ºèªã«ä¾¿åˆ©ï¼‰:")
    print("   diagnose_eaf_file('your_file.eaf')")

print()
print("ğŸ’¡ ãƒ†ã‚£ã‚¢åã®ç¢ºèªæ–¹æ³•:")
print("EAFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ†ã‚£ã‚¢åã‚’ç¢ºèªã™ã‚‹ã«ã¯:")
print("   diagnose_eaf_file('your_file.eaf')")
print("ã“ã‚Œã§åˆ©ç”¨å¯èƒ½ãªå…¨ãƒ†ã‚£ã‚¢åãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
print()
print("ğŸ“ ã‚ˆãã‚ã‚‹ãƒ†ã‚£ã‚¢åã®ãƒ‘ã‚¿ãƒ¼ãƒ³:")
print("- text@è©±è€…å (ä¾‹: text@KS, text@Speaker1)")
print("- morph@è©±è€…å (ä¾‹: morph@KS, morph@Speaker1)")
print("- gloss@è©±è€…å (ä¾‹: gloss@KS, gloss@Speaker1)")
print("- translation@è©±è€…å (ä¾‹: translation@KS, translation@Speaker1)")
print()
print("ğŸ“ LaTeXæ–‡æ›¸ã§ã®ä½¿ç”¨æ–¹æ³•:")
print("\\documentclass{article}")
print("\\usepackage[utf8]{inputenc}")
print("\\usepackage{CJKutf8}")
print("\\usepackage{gb4e}")
print("\\usepackage{tipa}")
print("\\usepackage{leipzig}  % Leipzig.styãƒ‘ãƒƒã‚±ãƒ¼ã‚¸")
print()
print("\\begin{document}")
print("\\input{your_file.eaf_gb4e_leipzig.tex}")
print("\\end{document}")
